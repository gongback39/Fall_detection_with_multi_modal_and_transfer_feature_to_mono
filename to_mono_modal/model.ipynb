{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "train_loader = create_dataloaders(\"../data/Training/01.원천데이터\", batch_size=6, test_ratio = 0, image_size=160, workers=16)\n",
    "val_loader = create_dataloaders( \"../data/Validation/01.원천데이터\", batch_size=6, test_ratio = 0, image_size=160, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['video_net.layer1.spatial_conv.weight', 'video_net.layer1.bn1.weight', 'video_net.layer1.bn1.bias', 'video_net.layer1.bn1.running_mean', 'video_net.layer1.bn1.running_var', 'video_net.layer1.temporal_conv.weight', 'video_net.layer1.bn2.weight', 'video_net.layer1.bn2.bias', 'video_net.layer1.bn2.running_mean', 'video_net.layer1.bn2.running_var', 'video_net.layer2.spatial_conv.weight', 'video_net.layer2.bn1.weight', 'video_net.layer2.bn1.bias', 'video_net.layer2.bn1.running_mean', 'video_net.layer2.bn1.running_var', 'video_net.layer2.temporal_conv.weight', 'video_net.layer2.bn2.weight', 'video_net.layer2.bn2.bias', 'video_net.layer2.bn2.running_mean', 'video_net.layer2.bn2.running_var', 'video_net.layer3.spatial_conv.weight', 'video_net.layer3.bn1.weight', 'video_net.layer3.bn1.bias', 'video_net.layer3.bn1.running_mean', 'video_net.layer3.bn1.running_var', 'video_net.layer3.temporal_conv.weight', 'video_net.layer3.bn2.weight', 'video_net.layer3.bn2.bias', 'video_net.layer3.bn2.running_mean', 'video_net.layer3.bn2.running_var', 'video_net.layer4.spatial_conv.weight', 'video_net.layer4.bn1.weight', 'video_net.layer4.bn1.bias', 'video_net.layer4.bn1.running_mean', 'video_net.layer4.bn1.running_var', 'video_net.layer4.temporal_conv.weight', 'video_net.layer4.bn2.weight', 'video_net.layer4.bn2.bias', 'video_net.layer4.bn2.running_mean', 'video_net.layer4.bn2.running_var', 'sensor_net.conv1.weight', 'sensor_net.conv1.bias', 'sensor_net.conv2.weight', 'sensor_net.conv2.bias', 'sensor_net.conv3.weight', 'sensor_net.conv3.bias', 'fc_sequence.0.weight', 'fc_sequence.0.bias', 'fc_sequence.2.weight', 'fc_sequence.2.bias'], unexpected_keys=['model_state_dict', 'optimizer_state_dict', 'loss'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multi_modal import MultiModalNet\n",
    "\n",
    "multi_modal_checkpoint = torch.load(\"multi_modal_24_12_15.pth\", map_location=DEVICE)\n",
    "teacher_model= MultiModalNet(num_classes=1).to(DEVICE)\n",
    "teacher_model.load_state_dict(multi_modal_checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from vision_model import R2Plus1D_Block\n",
    "\n",
    "class mono_modal(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(mono_modal, self).__init__()\n",
    "        self.layer1 = R2Plus1D_Block(3, 128, stride = 2)\n",
    "        self.layer2 = R2Plus1D_Block(128, 64, stride = 4)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "\n",
    "        self.fc_sequence = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc_sequence(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = mono_modal(num_classes=1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate=0.0001 \n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def distill_r2plus1d_blocks(teacher_model, student_model, video_input):\n",
    "    with torch.no_grad():\n",
    "        teacher_features1 = teacher_model.video_net.layer2(teacher_model.video_net.layer1(video_input))\n",
    "        teacher_features2 = teacher_model.video_net.layer4(teacher_model.video_net.layer3(teacher_features1))\n",
    "\n",
    "    student_features1 = student_model.layer1(video_input)\n",
    "    student_features2 = student_model.layer2(student_features1)\n",
    "\n",
    "    loss_r2p1d1 = F.mse_loss(student_features1, teacher_features1)\n",
    "    loss_r2p1d2 = F.mse_loss(student_features2, teacher_features2)\n",
    "\n",
    "    total_loss_r2p1d = (loss_r2p1d1 + loss_r2p1d2) / 2\n",
    "\n",
    "    return total_loss_r2p1d, teacher_features2, student_features2\n",
    "\n",
    "def distill_fc_sequence(teacher_model, student_model, video_features, sensor_features, temperature=3.0):\n",
    "    with torch.no_grad():\n",
    "        combined_teacher_features = torch.cat([video_features, sensor_features], dim=1) \n",
    "        teacher_output = teacher_model.fc_sequence(combined_teacher_features)\n",
    "\n",
    "    student_output = student_model.fc_sequence(video_features)\n",
    "\n",
    "    teacher_prob = torch.sigmoid(teacher_output / temperature)  \n",
    "    student_prob = torch.sigmoid(student_output / temperature) \n",
    "\n",
    "    teacher_dist = torch.cat([1 - teacher_prob, teacher_prob], dim=1)   \n",
    "    student_dist = torch.cat([1 - student_prob, student_prob], dim=1) \n",
    "\n",
    "    epsilon = 1e-7\n",
    "    student_log_dist = (student_dist + epsilon).log()\n",
    "    loss_fc = (temperature**2) * F.kl_div(\n",
    "        student_log_dist,  # log_prob of student\n",
    "        teacher_dist,       # prob of teacher\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "    return loss_fc, student_output\n",
    "\n",
    "def train(\n",
    "    teacher_model, \n",
    "    student_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    num_epochs=10, \n",
    "    alpha=0.7, \n",
    "    beta=0.3, \n",
    "    gamma=1.0,   # 추가된 gamma 가중치\n",
    "    temperature=3.0, \n",
    "    device=\"cuda\"\n",
    "):\n",
    "    teacher_model.eval()\n",
    "    student_model.to(device)\n",
    "\n",
    "    # BCEWithLogitsLoss 사용 예시 (이진 분류 가정)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for video_input, sensor_input, labels in pbar:\n",
    "            video_input, sensor_input, labels = video_input.to(device), sensor_input.to(device), labels.unsqueeze(1).float().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 교사 모델 특징 추출\n",
    "            video_features = teacher_model.video_net(video_input)\n",
    "            sensor_features = teacher_model.sensor_net(sensor_input) \n",
    "\n",
    "            # R2Plus1D 블록 지식 증류 손실\n",
    "            loss_r2p1d, teacher_features, student_features = distill_r2plus1d_blocks(\n",
    "                teacher_model, student_model, video_input\n",
    "            )\n",
    "\n",
    "            student_features = student_model.pool(student_features)\n",
    "            student_features = student_features.view(student_features.size(0), -1)\n",
    "\n",
    "            # fc_sequence 지식 증류 손실\n",
    "            loss_fc, student_output = distill_fc_sequence(\n",
    "                teacher_model, student_model, video_features, sensor_features, temperature\n",
    "            )\n",
    "\n",
    "            # 분류 손실\n",
    "            classification_loss = criterion(student_output, labels)\n",
    "\n",
    "            # 총 손실 계산: alpha, beta, gamma 가중치 적용\n",
    "            loss = alpha * loss_r2p1d + beta * loss_fc + gamma * classification_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "            # 정확도 계산\n",
    "            # 이진 분류라면 다음과 같이 sigmoid 후 thresholding 필요\n",
    "            # 예: predicted = (torch.sigmoid(student_output) > 0.5).long()\n",
    "            # 여기서는 다중 클래스 가정으로 torch.max 사용\n",
    "            predicted = (torch.sigmoid(student_output) > 0.5).long()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = correct / total * 100\n",
    "        print(f\"Training Loss: {total_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for video_input, _, labels in val_loader:\n",
    "                video_input, labels = video_input.to(device), labels.unsqueeze(1).float().to(DEVICE)\n",
    "                \n",
    "                student_output = student_model(video_input)\n",
    "                \n",
    "                batch_val_loss = criterion(student_output, labels).item()\n",
    "                val_loss += batch_val_loss\n",
    "\n",
    "                predicted = (torch.sigmoid(student_output) > 0.5).long()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy().flatten())\n",
    "                all_preds.extend(predicted.cpu().numpy().flatten())\n",
    "\n",
    "        val_accuracy = correct / total * 100\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        # Precision, Recall, F1-Score 계산\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3022/3022 [26:23<00:00,  1.91it/s, loss=1.49] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2907.7771, Training Accuracy: 97.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 213.6727, Validation Accuracy: 80.15%\n",
      "Confusion Matrix:\n",
      " [[ 385  183]\n",
      " [ 268 1436]]\n",
      "Precision: 0.89, Recall: 0.84, F1-Score: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3022/3022 [26:20<00:00,  1.91it/s, loss=1.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2700.9620, Training Accuracy: 99.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 194.9082, Validation Accuracy: 75.57%\n",
      "Confusion Matrix:\n",
      " [[  14  554]\n",
      " [   1 1703]]\n",
      "Precision: 0.75, Recall: 1.00, F1-Score: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 60/3022 [00:40<33:15,  1.48it/s, loss=0.64]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(teacher_model, student_model, train_loader, val_loader, optimizer, num_epochs, alpha, beta, gamma, temperature, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 정확도 계산\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 이진 분류라면 다음과 같이 sigmoid 후 thresholding 필요\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# 예: predicted = (torch.sigmoid(student_output) > 0.5).long()\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# 여기서는 다중 클래스 가정으로 torch.max 사용\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=10,\n",
    "    alpha=0.005,\n",
    "    beta=0.005,\n",
    "    gamma = 0.99,\n",
    "    temperature=5.0,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: student_model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m }, model_save_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and parameters saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "model_save_path = 'model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': student_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, model_save_path)\n",
    "print(f\"Model and parameters saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
